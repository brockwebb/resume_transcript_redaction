{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012f6bc2-291b-48e2-8418-c35d2ab87308",
   "metadata": {},
   "source": [
    "# Resume and Transcript Redaction\n",
    "___\n",
    "## Step 1. Install dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e865bee8-068f-44f8-b7c4-87e752479f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if needed.\n",
    "#!pip install -r requirements.txt\n",
    "#!pip install streamlit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c31091d-5782-4f63-a055-43223d24a9e4",
   "metadata": {},
   "source": [
    "## Step 2. Build a catalogue of resumes for creating test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb1372ef-da73-4d24-94f4-d978bcb7b92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume catalog saved to ../data/resume_sampling_catalogue.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>folder</th>\n",
       "      <th>filename</th>\n",
       "      <th>absolute_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGRICULTURE</td>\n",
       "      <td>37201447.pdf</td>\n",
       "      <td>../data/AGRICULTURE/37201447.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGRICULTURE</td>\n",
       "      <td>12674256.pdf</td>\n",
       "      <td>../data/AGRICULTURE/12674256.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGRICULTURE</td>\n",
       "      <td>29968330.pdf</td>\n",
       "      <td>../data/AGRICULTURE/29968330.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGRICULTURE</td>\n",
       "      <td>81042872.pdf</td>\n",
       "      <td>../data/AGRICULTURE/81042872.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGRICULTURE</td>\n",
       "      <td>20006992.pdf</td>\n",
       "      <td>../data/AGRICULTURE/20006992.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        folder      filename                     absolute_path\n",
       "0  AGRICULTURE  37201447.pdf  ../data/AGRICULTURE/37201447.pdf\n",
       "1  AGRICULTURE  12674256.pdf  ../data/AGRICULTURE/12674256.pdf\n",
       "2  AGRICULTURE  29968330.pdf  ../data/AGRICULTURE/29968330.pdf\n",
       "3  AGRICULTURE  81042872.pdf  ../data/AGRICULTURE/81042872.pdf\n",
       "4  AGRICULTURE  20006992.pdf  ../data/AGRICULTURE/20006992.pdf"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def scan_folders_to_catalogue(root_folder, output_csv_path):\n",
    "    \"\"\"\n",
    "    Scans the /data folder structure starting from the root folder and creates a CSV file\n",
    "    cataloging all files in the subfolders.\n",
    "\n",
    "    Assumes you've extracted all the folders from the Kaggle dataset: \n",
    "    https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset\n",
    "\n",
    "    Args:\n",
    "        root_folder (str): The path to the root folder to scan.\n",
    "        output_csv_path (str): Path to save the resulting catalog CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of the scanned file information.\n",
    "    \"\"\"\n",
    "    # List to hold file details\n",
    "    file_data = []\n",
    "\n",
    "    # Walk through all subdirectories and files\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            # Skip unnecessary files\n",
    "            if filename in [\".DS_Store\", \".keep\", \"resume_sampling_catalogue.csv\"]:\n",
    "                continue         \n",
    "            \n",
    "            # Construct file details\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            relative_path = os.path.relpath(file_path, root_folder)\n",
    "            file_data.append({\n",
    "                \"folder\": os.path.dirname(relative_path),  # Relative folder path\n",
    "                \"filename\": filename,                     # File name\n",
    "                \"absolute_path\": file_path                # Absolute file path\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    file_df = pd.DataFrame(file_data)\n",
    "\n",
    "    # Save the catalog to CSV\n",
    "    file_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Resume catalog saved to {output_csv_path}\")\n",
    "\n",
    "    return file_df\n",
    "\n",
    "# Example Usage\n",
    "root_folder = \"../data\"  # Replace with the root directory containing the subfolders\n",
    "output_csv_path = \"../data/resume_sampling_catalogue.csv\"  # Path to save the catalog CSV\n",
    "\n",
    "# Run the function to generate the catalogue\n",
    "resume_catalog = scan_folders_to_catalogue(root_folder, output_csv_path)\n",
    "\n",
    "# Display the first few rows of the catalog\n",
    "resume_catalog.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb32385-8029-4a7b-89e1-90f3b0af45dd",
   "metadata": {},
   "source": [
    "# Step 3. Create a random sample of 20 resumes for the Input Folder\n",
    "Here’s a clean and modular code block to create the `../data/redact_input` folder and populate it with a test sample from the `resume_sampling_catalogue.csv`. This keeps the sampling and redaction workflows separate, allowing you to modify the input folder manually as needed.\n",
    "\n",
    "---\n",
    "### Explanation:\n",
    "1. **Purpose**:\n",
    "   - Populates `../data/redact_input` with a random sample of PDFs from `resume_sampling_catalogue.csv`.\n",
    "   - Allows users to add or modify files in the `redact_input` folder later.\n",
    "\n",
    "2. **Random Sampling**:\n",
    "   - Reads `resume_sampling_catalogue.csv` to randomly select files.\n",
    "   - If the catalogue has fewer files than the `SAMPLE_SIZE`, it copies all files.\n",
    "\n",
    "3. **Copying Files**:\n",
    "   - Creates the `redact_input` folder if it doesn’t exist.\n",
    "   - Copies sampled PDFs to `../data/redact_input`.\n",
    "\n",
    "4. **Print Statements**:\n",
    "   - Tracks progress for better visibility.\n",
    "\n",
    "---\n",
    "\n",
    "### Directory Structure After Execution:\n",
    "- `../data/redact_input/`: Contains the sampled files, ready for redaction.\n",
    "- `../data/resume_sampling_catalogue.csv`: The source catalogue file remains untouched.\n",
    "\n",
    "This approach ensures that you can **manually adjust the contents** of `../data/redact_input` before running the redaction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be8140-5c6e-4a33-855f-efab6311e6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def populate_redact_input(catalogue_path, input_folder, sample_size):\n",
    "    \"\"\"\n",
    "    Populates the redact_input folder with a random sample of PDFs from the catalogue.\n",
    "    Allows users to manually adjust the input folder later if needed.\n",
    "\n",
    "    Args:\n",
    "        catalogue_path (str): Path to the resume_sampling_catalogue.csv file.\n",
    "        input_folder (str): Path to the redact_input folder to populate.\n",
    "        sample_size (int): Number of PDFs to copy to the input folder.\n",
    "    \"\"\"\n",
    "    # Read the catalogue\n",
    "    print(f\"Loading file catalogue from {catalogue_path}...\")\n",
    "    file_catalogue = pd.read_csv(catalogue_path)\n",
    "\n",
    "    # Ensure the input folder exists\n",
    "    os.makedirs(input_folder, exist_ok=True)\n",
    "\n",
    "    # Select a random sample from the catalogue\n",
    "    print(f\"Selecting a random sample of {sample_size} PDFs...\")\n",
    "    if len(file_catalogue) <= sample_size:\n",
    "        sample_catalogue = file_catalogue\n",
    "    else:\n",
    "        sample_catalogue = file_catalogue.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    # Copy the sampled PDFs to the redact_input folder\n",
    "    print(f\"Copying sampled files to {input_folder}...\")\n",
    "    for _, row in sample_catalogue.iterrows():\n",
    "        src_path = row['absolute_path']\n",
    "        filename = os.path.basename(src_path)\n",
    "        dest_path = os.path.join(input_folder, filename)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "    print(f\"Populated {input_folder} with {len(sample_catalogue)} files.\")\n",
    "\n",
    "# Example Usage\n",
    "CATALOGUE_PATH = \"../data/resume_sampling_catalogue.csv\"\n",
    "INPUT_FOLDER = \"../data/redact_input\"\n",
    "SAMPLE_SIZE = 20\n",
    "\n",
    "populate_redact_input(CATALOGUE_PATH, INPUT_FOLDER, SAMPLE_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c21034-fa19-4ffe-b6c3-324c54cd4e3c",
   "metadata": {},
   "source": [
    "# Step 4.Redaction Workflow\n",
    "\n",
    "### Features:\n",
    "1. **Test Mode**:\n",
    "   - Controlled via the `TEST_MODE` parameter.\n",
    "   - `True`: Adds semi-transparent grey redaction boxes.\n",
    "   - `False`: Applies solid black redaction boxes permanently.\n",
    "\n",
    "2. **Output Folder Management**:\n",
    "   - Checks if `../data/redact_output` exists and contains files.\n",
    "   - Prompts the user to clear it before proceeding.\n",
    "\n",
    "3. **Clear Feedback**:\n",
    "   - Summarizes the redaction process for all PDFs.\n",
    "   - Tracks progress using a `tqdm` progress bar.\n",
    "\n",
    "4. **Unified Input and Output**:\n",
    "   - Processes files from `../data/redact_input`.\n",
    "   - Outputs files to `../data/redact_output`.\n",
    "\n",
    "---\n",
    "\n",
    "## To use:\n",
    "1. Place files to redact in `../data/redact_input`. (use sample creator above or add a custom set)\n",
    "2. Run the script. If `../data/redact_output` contains files, you’ll be prompted to clear it.\n",
    "3. Outputs:\n",
    "   - Redacted files are saved in `../data/redact_output` with `_output` appended to their filenames.\n",
    "   - A summary of the process is printed.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11bf507f-91cb-4eb8-83c4-b12c49c38677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The folder '../data/redact_output' is not empty. Do you want to clear it? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared all files from '../data/redact_output'.\n",
      "Redacting PDFs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████████████████████| 1/1 [00:02<00:00,  2.08s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      " Redaction Summary \n",
      "============================================================\n",
      "File: brock-webb-resume-Sep2022.rtf.pdf -> brock-webb-resume-Sep2022.rtf_output.pdf\n",
      "  Words: 1199, Redacted: 68\n",
      "------------------------------------------------------------\n",
      "Processed 1 PDFs\n",
      "Total words processed: 1199\n",
      "Total words redacted: 68\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm  # For progress bar\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "\n",
    "##############################################################################\n",
    "# CONFIGURATION\n",
    "##############################################################################\n",
    "INPUT_FOLDER = \"../data/redact_input\"\n",
    "OUTPUT_FOLDER = \"../data/redact_output\"\n",
    "TEST_MODE = True  # Set to True for test mode (transparent boxes), False for finalized redactions\n",
    "LANGUAGE = \"en\"   # Language code for Presidio\n",
    "\n",
    "# Initialize Presidio analyzer engine\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "##############################################################################\n",
    "# HELPER FUNCTIONS\n",
    "##############################################################################\n",
    "\n",
    "def clear_output_folder(output_folder):\n",
    "    \"\"\"\n",
    "    Clears the output folder after user confirmation.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_folder) and os.listdir(output_folder):\n",
    "        user_input = input(f\"The folder '{output_folder}' is not empty. Do you want to clear it? (yes/no): \")\n",
    "        if user_input.lower() in [\"yes\", \"y\"]:\n",
    "            for file in os.listdir(output_folder):\n",
    "                file_path = os.path.join(output_folder, file)\n",
    "                os.remove(file_path)\n",
    "            print(f\"Cleared all files from '{output_folder}'.\")\n",
    "        else:\n",
    "            print(\"Operation aborted. Please clear the folder and re-run the script.\")\n",
    "            exit()\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "def detect_pii_in_word(word_text):\n",
    "    \"\"\"\n",
    "    Use Presidio to analyze a single word (string).\n",
    "    Returns True if the word is detected as PII, otherwise False.\n",
    "    \"\"\"\n",
    "    if not word_text.strip():\n",
    "        return False\n",
    "    results = analyzer.analyze(text=word_text, language=LANGUAGE)\n",
    "    return len(results) > 0\n",
    "\n",
    "def redact_pii_in_pdf(input_pdf_path, output_pdf_path, test_mode=True):\n",
    "    \"\"\"\n",
    "    Opens a PDF, detects PII word-by-word, and applies redaction.\n",
    "    - In test mode: Transparent redaction boxes are added.\n",
    "    - In final mode: Solid black redactions are permanently applied.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(input_pdf_path)\n",
    "    total_words = 0\n",
    "    redacted_words = 0\n",
    "\n",
    "    for page_index in range(len(doc)):\n",
    "        page = doc[page_index]\n",
    "        wordlist = page.get_text(\"words\")\n",
    "        redact_areas = []\n",
    "\n",
    "        for w in wordlist:\n",
    "            text = w[4]\n",
    "            total_words += 1\n",
    "            if detect_pii_in_word(text):\n",
    "                x0, y0, x1, y1 = w[0], w[1], w[2], w[3]\n",
    "                rect = fitz.Rect(x0, y0, x1, y1)\n",
    "                redact_areas.append(rect)\n",
    "                redacted_words += 1\n",
    "\n",
    "        # Add redaction annotations to the page\n",
    "        for rect in redact_areas:\n",
    "            if test_mode:\n",
    "                # Test mode: Transparent grey boxes\n",
    "                page.add_redact_annot(rect, fill=(192, 192, 192, 128))  # Semi-transparent grey\n",
    "            else:\n",
    "                # Final mode: Solid black boxes\n",
    "                page.add_redact_annot(rect, fill=(0, 0, 0))  # Solid black\n",
    "\n",
    "        # Apply the redactions (only has an effect in final mode)\n",
    "        if not test_mode:\n",
    "            page.apply_redactions()\n",
    "\n",
    "    # Save the document\n",
    "    doc.save(output_pdf_path, garbage=4, deflate=True)\n",
    "    doc.close()\n",
    "\n",
    "    return {\"total_words\": total_words, \"redacted_words\": redacted_words}\n",
    "\n",
    "##############################################################################\n",
    "# MAIN WORKFLOW\n",
    "##############################################################################\n",
    "\n",
    "def main():\n",
    "    # 1) Check and clear the output folder\n",
    "    clear_output_folder(OUTPUT_FOLDER)\n",
    "\n",
    "    # 2) Get all input PDFs\n",
    "    input_pdfs = [os.path.join(INPUT_FOLDER, f) for f in os.listdir(INPUT_FOLDER) if f.lower().endswith(\".pdf\")]\n",
    "    if not input_pdfs:\n",
    "        print(f\"No PDF files found in '{INPUT_FOLDER}'. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 3) Process each PDF and redact PII\n",
    "    print(\"Redacting PDFs...\")\n",
    "    overall_summary = []\n",
    "    for pdf_path in tqdm(input_pdfs, desc=\"Processing PDFs\", unit=\"file\"):\n",
    "        base_name = os.path.basename(pdf_path)\n",
    "        base, ext = os.path.splitext(base_name)\n",
    "        redacted_filename = f\"{base}_output{ext}\"\n",
    "        redacted_filepath = os.path.join(OUTPUT_FOLDER, redacted_filename)\n",
    "        summary = redact_pii_in_pdf(pdf_path, redacted_filepath, test_mode=TEST_MODE)\n",
    "        overall_summary.append({\n",
    "            \"input_pdf\": base_name,\n",
    "            \"redacted_pdf\": os.path.basename(redacted_filepath),\n",
    "            \"total_words\": summary[\"total_words\"],\n",
    "            \"redacted_words\": summary[\"redacted_words\"]\n",
    "        })\n",
    "\n",
    "    # 4) Print summary statistics\n",
    "    print(\"=\" * 60)\n",
    "    print(\" Redaction Summary \")\n",
    "    print(\"=\" * 60)\n",
    "    total_documents = len(overall_summary)\n",
    "    total_words_processed = sum(d['total_words'] for d in overall_summary)\n",
    "    total_redacted = sum(d['redacted_words'] for d in overall_summary)\n",
    "\n",
    "    for doc_summary in overall_summary:\n",
    "        print(f\"File: {doc_summary['input_pdf']} -> {doc_summary['redacted_pdf']}\")\n",
    "        print(f\"  Words: {doc_summary['total_words']}, Redacted: {doc_summary['redacted_words']}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"Processed {total_documents} PDFs\")\n",
    "    print(f\"Total words processed: {total_words_processed}\")\n",
    "    print(f\"Total words redacted: {total_redacted}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859abbf6-fa70-4a01-a45b-fa8ade8f67fe",
   "metadata": {},
   "source": [
    "# Step 5 Extending Functionality using jobBERT \n",
    "\n",
    "## 5.1 Set up a Hugging Face environment \n",
    "1. install required libraries (M1 has special \n",
    "2. install ipykernel\n",
    "3. add to jupyter `python -m ipykernel install --user --name=hf_env --display-name \"Python (hf_env)\"`\n",
    "4. restart jupyter\n",
    "\n",
    "### Packages (in order)\n",
    "1. torch\n",
    "1. torchvision \n",
    "1. torchaudio\n",
    "1. transformers\n",
    "1. tf-keras\n",
    "1. pandas\n",
    "1. numpy\n",
    "1. matplotlib\n",
    "\n",
    "### M1/M2/M3 Macs ... \n",
    "- You have to use the CPU (not GPU) no CUDA support.\n",
    "- torch, torchvision, torchaudio are different\n",
    "- `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84e4ef2e-ea20-43d6-bdb2-9122a3a85917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac M1 install:\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a61b3b-74aa-4cb5-9e65-6869f9d2cd0d",
   "metadata": {},
   "source": [
    "___\n",
    "## 5.2 JobBERT Setup\n",
    "- load and save the JobBERT model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4b18f11-3a57-4215-ba28-ed172041d903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6d84c4a92e4ea8bc0d2c7792a01cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588a2125bae54d098c72b50ee6383b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/603 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4acac4ab2d54aabab29ce18cdc74ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a351493e85c045bc99c53329c33d56db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cd4d7857af4c5b9bb0cb355de0ff4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jjzha/jobbert-base-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobBERT model saved locally.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Download and save JobBERT locally (one-time)\n",
    "model_name = \"jjzha/jobbert-base-cased\"  # Replace with the correct JobBERT model\n",
    "save_directory = \"./jobbert_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)\n",
    "print(\"JobBERT model saved locally.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7cd983-aaa6-49c1-b0fb-8ffe9b098c19",
   "metadata": {},
   "source": [
    "## 5.2.1 Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "244c4549-f0be-4190-9168-8c897404c7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobBERT loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load JobBERT locally\n",
    "model_directory = \"./jobbert_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "model = AutoModel.from_pretrained(model_directory)\n",
    "\n",
    "print(\"JobBERT loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9ec64-d9bb-4275-986e-4f8cf78124b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313e149f-4f42-4c94-a549-bee618cf5206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The folder '../data/redact_output' is not empty. Clear it? (yes/no):  yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " Redaction Summary \n",
      "============================================================\n",
      "\n",
      "File: brock-webb-resume-Sep2022.rtf.pdf → brock-webb-resume-Sep2022.rtf_redacted.pdf\n",
      "  Words processed: 1,199\n",
      "  Words redacted:  67\n",
      "  Redaction rate:  5.59%\n",
      "------------------------------------------------------------\n",
      "\n",
      "Overall Statistics:\n",
      "Documents processed: 1\n",
      "Total words processed: 1,199\n",
      "Total words redacted: 67\n",
      "Overall redaction rate: 5.59%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern\n",
    "from presidio_analyzer.nlp_engine import NlpEngine\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Any\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import regex as re\n",
    "import sys\n",
    "\n",
    "##############################################################################\n",
    "# CONFIGURATION\n",
    "##############################################################################\n",
    "@dataclass\n",
    "class Config:\n",
    "    input_folder: str\n",
    "    output_folder: str\n",
    "    model_directory: str\n",
    "    test_mode: bool\n",
    "    language: str\n",
    "    confidence_threshold: float\n",
    "    batch_size: int\n",
    "    cache_enabled: bool\n",
    "    pii_types: List[str]\n",
    "    redaction_options: Dict[str, Any]\n",
    "    logging: Dict[str, Any]\n",
    "\n",
    "\n",
    "def load_config(config_path: str = \"config.yaml\") -> Config:\n",
    "    \"\"\"Load configuration from YAML file\"\"\"\n",
    "    default_config = {\n",
    "        \"input_folder\": \"../data/redact_input\",\n",
    "        \"output_folder\": \"../data/redact_output\",\n",
    "        \"model_directory\": \"../models/jobbert_model\",\n",
    "        \"test_mode\": True,\n",
    "        \"language\": \"en\",\n",
    "        \"confidence_threshold\": 0.75,\n",
    "        \"batch_size\": 32,\n",
    "        \"cache_enabled\": True,\n",
    "        \"pii_types\": [\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\", \"LOCATION\"],\n",
    "        \"redaction_options\": {\n",
    "            \"minimum_confidence_score\": 0.65,\n",
    "            \"context_words_before\": 2,\n",
    "            \"context_words_after\": 2,\n",
    "            \"preserve_formatting\": True,\n",
    "            \"redaction_char\": \"█\",\n",
    "        },\n",
    "        \"logging\": {\n",
    "            \"level\": \"INFO\",\n",
    "            \"file\": \"redaction.log\",\n",
    "            \"console_output\": True,\n",
    "            \"detailed_summary\": True,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            config_data = yaml.safe_load(f)\n",
    "            default_config.update(config_data)\n",
    "    \n",
    "    return Config(**default_config)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# LOGGING SETUP\n",
    "##############################################################################\n",
    "def setup_logging(config: Config):\n",
    "    \"\"\"Setup logging with proper handlers and avoid duplicate outputs\"\"\"\n",
    "    # First, remove any existing handlers\n",
    "    root = logging.getLogger()\n",
    "    for handler in root.handlers[:]:\n",
    "        root.removeHandler(handler)\n",
    "    \n",
    "    # Create formatter\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Setup handlers based on config\n",
    "    handlers = []\n",
    "    if config.logging.get(\"console_output\", True):\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(formatter)\n",
    "        handlers.append(console_handler)\n",
    "    \n",
    "    if config.logging.get(\"file\"):\n",
    "        file_handler = logging.FileHandler(config.logging[\"file\"])\n",
    "        file_handler.setFormatter(formatter)\n",
    "        handlers.append(file_handler)\n",
    "\n",
    "    # Configure root logger\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, config.logging.get(\"level\", \"ERROR\").upper(), logging.ERROR),\n",
    "        handlers=handlers\n",
    "    )\n",
    "\n",
    "    # Enhanced logging suppression for external libraries\n",
    "    logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"pytorch\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"presidio\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"presidio.analyzer\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"presidio.analyzer.analyzer_engine\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"presidio.analyzer.pattern\").setLevel(logging.ERROR)\n",
    "    logging.getLogger(\"presidio.analyzer.pattern_recognizer\").setLevel(logging.ERROR)\n",
    "\n",
    "##############################################################################\n",
    "# MODEL MANAGEMENT\n",
    "##############################################################################\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_directory: str, device: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the ModelManager to handle JobBERT and Presidio integration.\n",
    "        \"\"\"\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.analyzer = None\n",
    "        self.word_cache = defaultdict(dict)\n",
    "        \n",
    "        # Completely suppress Presidio logging during initialization\n",
    "        logging.getLogger(\"presidio\").disabled = True\n",
    "        \n",
    "        self.custom_recognizers = self.setup_custom_recognizers()\n",
    "        self.load_models(model_directory)\n",
    "        \n",
    "        # Re-enable logging after initialization if needed\n",
    "        logging.getLogger(\"presidio\").disabled = False\n",
    "\n",
    "    def load_models(self, model_directory: str):\n",
    "        \"\"\"\n",
    "        Load JobBERT and initialize Presidio Analyzer with custom recognizers.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load JobBERT model and tokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "            self.model = AutoModel.from_pretrained(model_directory).to(self.device)\n",
    "\n",
    "            # Initialize Presidio Analyzer\n",
    "            self.analyzer = AnalyzerEngine()\n",
    "\n",
    "            # Add custom recognizers to the Presidio registry\n",
    "            for recognizer in self.custom_recognizers:\n",
    "                self.analyzer.registry.add_recognizer(recognizer)\n",
    "\n",
    "            logging.info(\"Models loaded successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def setup_custom_recognizers(self) -> List[PatternRecognizer]:\n",
    "        \"\"\"\n",
    "        Define and return custom recognizers for specific PII types.\n",
    "        \"\"\"\n",
    "        custom_recognizers = []\n",
    "\n",
    "        # Example: Custom recognizer for dates\n",
    "        date_patterns = [\n",
    "            Pattern(\n",
    "                name=\"date_numeric\",\n",
    "                regex=r\"(\\d{1,2}/\\d{4}|\\d{4})–?(Present|\\d{1,2}/\\d{4})\",\n",
    "                score=0.85\n",
    "            ),\n",
    "            Pattern(\n",
    "                name=\"date_alphanumeric\",\n",
    "                regex=r\"(?i)(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{4}\",\n",
    "                score=0.85\n",
    "            )\n",
    "        ]\n",
    "        date_recognizer = PatternRecognizer(\n",
    "            supported_entity=\"DATE\",\n",
    "            patterns=date_patterns,\n",
    "            context=[\"date\", \"period\", \"timeline\"]\n",
    "        )\n",
    "        custom_recognizers.append(date_recognizer)\n",
    "\n",
    "        # Add more custom recognizers as needed (e.g., certifications, financial terms)\n",
    "\n",
    "        return custom_recognizers\n",
    "\n",
    "    def detect_pii_in_word(self, word_text: str, config: Config) -> bool:\n",
    "        \"\"\"\n",
    "        Detect PII in a given word using Presidio and custom recognizers.\n",
    "        Args:\n",
    "            word_text (str): The word to check for PII.\n",
    "            config (Config): Configuration object with relevant settings.\n",
    "        Returns:\n",
    "            bool: True if the word is detected as PII, otherwise False.\n",
    "        \"\"\"\n",
    "        if not word_text.strip():\n",
    "            return False\n",
    "\n",
    "        # Temporarily disable logging during analysis\n",
    "        logging.getLogger(\"presidio\").disabled = True\n",
    "        try:\n",
    "            results = self.analyzer.analyze(\n",
    "                text=word_text,\n",
    "                language=config.language\n",
    "            )\n",
    "        finally:\n",
    "            # Re-enable logging after analysis\n",
    "            logging.getLogger(\"presidio\").disabled = False\n",
    "\n",
    "        # Check if any recognized entity matches PII types with sufficient confidence\n",
    "        return any(\n",
    "            result.entity_type in config.pii_types\n",
    "            and result.score >= config.redaction_options.get(\"minimum_confidence_score\", 0.5)\n",
    "            for result in results\n",
    "        )\n",
    "\n",
    "    def validate_with_jobbert(self, texts: List[str], config: Config) -> List[bool]:\n",
    "        \"\"\"\n",
    "        Validate multiple texts with JobBERT embeddings.\n",
    "        Args:\n",
    "            texts (List[str]): List of text fragments to validate.\n",
    "            config (Config): Configuration object.\n",
    "        Returns:\n",
    "            List[bool]: List of validation results for each text.\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            return []\n",
    "\n",
    "        # Check cache if enabled\n",
    "        if config.cache_enabled:\n",
    "            cached_results = [self.word_cache.get(text, None) for text in texts]\n",
    "            if all(result is not None for result in cached_results):\n",
    "                return cached_results\n",
    "\n",
    "        # Process uncached texts\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        confidence_scores = [torch.norm(emb).item() for emb in embeddings]\n",
    "        results = [score > config.confidence_threshold for score in confidence_scores]\n",
    "\n",
    "        # Update cache\n",
    "        if config.cache_enabled:\n",
    "            for text, result in zip(texts, results):\n",
    "                self.word_cache[text] = result\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# PDF PROCESSING\n",
    "##############################################################################\n",
    "class PDFProcessor:\n",
    "    def __init__(self, model_manager: ModelManager, config: Config):\n",
    "        self.model_manager = model_manager\n",
    "        self.config = config\n",
    "\n",
    "    def process_page(self, page: fitz.Page) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single PDF page for PII redaction.\"\"\"\n",
    "        wordlist = page.get_text(\"words\")\n",
    "        redact_areas = []\n",
    "        batch_texts = []\n",
    "        batch_coords = []\n",
    "        context_words_before = self.config.redaction_options.get(\"context_words_before\", 0)\n",
    "        context_words_after = self.config.redaction_options.get(\"context_words_after\", 0)\n",
    "\n",
    "        for idx, word in enumerate(wordlist):\n",
    "            text = word[4]\n",
    "            if self.model_manager.detect_pii_in_word(text, self.config):\n",
    "                # Include context words\n",
    "                context_start = max(0, idx - context_words_before)\n",
    "                context_end = min(len(wordlist), idx + context_words_after + 1)\n",
    "                context = \" \".join([w[4] for w in wordlist[context_start:context_end]])\n",
    "                batch_texts.append(context)\n",
    "                batch_coords.append((word[0], word[1], word[2], word[3]))\n",
    "\n",
    "        # Validate with JobBERT\n",
    "        if batch_texts:\n",
    "            validation_results = self.model_manager.validate_with_jobbert(batch_texts, self.config)\n",
    "            for (coords, should_redact) in zip(batch_coords, validation_results):\n",
    "                if should_redact:\n",
    "                    redact_areas.append(fitz.Rect(*coords))\n",
    "\n",
    "        return {\"redact_areas\": redact_areas, \"word_count\": len(wordlist)}\n",
    "\n",
    "    def redact_pdf(self, input_path: str, output_path: str) -> Dict[str, int]:\n",
    "        \"\"\"Redact PII from a PDF document.\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(input_path)\n",
    "            total_words = 0\n",
    "            redacted_words = 0\n",
    "\n",
    "            for page_index in range(len(doc)):\n",
    "                page = doc[page_index]\n",
    "                result = self.process_page(page)\n",
    "                total_words += result[\"word_count\"]\n",
    "                redacted_words += len(result[\"redact_areas\"])\n",
    "\n",
    "                # Apply redactions\n",
    "                for rect in result[\"redact_areas\"]:\n",
    "                    fill_color = (192, 192, 192, 128) if self.config.test_mode else (0, 0, 0)\n",
    "                    page.add_redact_annot(rect, fill=fill_color)\n",
    "                if not self.config.test_mode:\n",
    "                    page.apply_redactions()\n",
    "\n",
    "            doc.save(output_path, garbage=4, deflate=True)\n",
    "            doc.close()\n",
    "            return {\"total_words\": total_words, \"redacted_words\": redacted_words}\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing PDF {input_path}: {str(e)}\")\n",
    "            return {\"total_words\": 0, \"redacted_words\": 0}\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# FILE MANAGEMENT\n",
    "##############################################################################\n",
    "class FileManager:\n",
    "    @staticmethod\n",
    "    def clear_output_folder(output_folder: str):\n",
    "        \"\"\"Clear output folder with user confirmation\"\"\"\n",
    "        if os.path.exists(output_folder) and os.listdir(output_folder):\n",
    "            user_input = input(f\"The folder '{output_folder}' is not empty. Clear it? (yes/no): \")\n",
    "            if user_input.lower() in [\"yes\", \"y\"]:\n",
    "                for file in os.listdir(output_folder):\n",
    "                    os.remove(os.path.join(output_folder, file))\n",
    "                logging.info(f\"Cleared all files from '{output_folder}'\")\n",
    "            else:\n",
    "                logging.info(\"Operation aborted\")\n",
    "                exit()\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_pdf_files(input_folder: str) -> List[str]:\n",
    "        \"\"\"Get list of PDF files from input folder\"\"\"\n",
    "        return [os.path.join(input_folder, f) for f in os.listdir(input_folder) \n",
    "                if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "##############################################################################\n",
    "# MAIN WORKFLOW\n",
    "##############################################################################\n",
    "def main():\n",
    "    # Load configuration\n",
    "    config = load_config()\n",
    "\n",
    "    # Setup logging with reduced output\n",
    "    setup_logging(config)\n",
    "\n",
    "    try:\n",
    "        # Initialize components\n",
    "        logging.info(\"Initializing model manager...\")\n",
    "        model_manager = ModelManager(config.model_directory)\n",
    "        pdf_processor = PDFProcessor(model_manager, config)\n",
    "\n",
    "        # Prepare folders\n",
    "        FileManager.clear_output_folder(config.output_folder)\n",
    "        input_pdfs = FileManager.get_pdf_files(config.input_folder)\n",
    "\n",
    "        if not input_pdfs:\n",
    "            logging.warning(f\"No PDF files found in '{config.input_folder}'\")\n",
    "            return\n",
    "\n",
    "        # Process PDFs\n",
    "        logging.info(f\"Found {len(input_pdfs)} PDF files to process\")\n",
    "        overall_summary = []\n",
    "\n",
    "        # Disable tqdm in non-interactive environments\n",
    "        disable_tqdm = not sys.stdout.isatty()\n",
    "        \n",
    "        with tqdm(total=len(input_pdfs), \n",
    "                 desc=\"Processing PDFs\", \n",
    "                 unit=\"file\",\n",
    "                 disable=disable_tqdm) as pbar:\n",
    "            \n",
    "            for pdf_path in input_pdfs:\n",
    "                try:\n",
    "                    base_name = os.path.basename(pdf_path)\n",
    "                    output_filename = f\"{os.path.splitext(base_name)[0]}_redacted.pdf\"\n",
    "                    output_path = os.path.join(config.output_folder, output_filename)\n",
    "\n",
    "                    logging.debug(f\"Processing: {base_name}\")\n",
    "                    summary = pdf_processor.redact_pdf(pdf_path, output_path)\n",
    "                    \n",
    "                    overall_summary.append({\n",
    "                        \"input_pdf\": base_name,\n",
    "                        \"redacted_pdf\": output_filename,\n",
    "                        **summary\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing {base_name}: {str(e)}\")\n",
    "                    continue\n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\" Redaction Summary \")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        total_documents = len(overall_summary)\n",
    "        total_words = sum(d['total_words'] for d in overall_summary)\n",
    "        total_redacted = sum(d['redacted_words'] for d in overall_summary)\n",
    "\n",
    "        # Print detailed summary if enabled\n",
    "        if config.logging.get(\"detailed_summary\", True):\n",
    "            for doc_summary in overall_summary:\n",
    "                print(f\"\\nFile: {doc_summary['input_pdf']} → {doc_summary['redacted_pdf']}\")\n",
    "                print(f\"  Words processed: {doc_summary['total_words']:,}\")\n",
    "                print(f\"  Words redacted:  {doc_summary['redacted_words']:,}\")\n",
    "                redaction_rate = (doc_summary['redacted_words'] / doc_summary['total_words'] * 100 \n",
    "                                if doc_summary['total_words'] > 0 else 0)\n",
    "                print(f\"  Redaction rate:  {redaction_rate:.2f}%\")\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "        # Print overall statistics\n",
    "        print(\"\\nOverall Statistics:\")\n",
    "        print(f\"Documents processed: {total_documents:,}\")\n",
    "        print(f\"Total words processed: {total_words:,}\")\n",
    "        print(f\"Total words redacted: {total_redacted:,}\")\n",
    "        overall_rate = (total_redacted / total_words * 100 if total_words > 0 else 0)\n",
    "        print(f\"Overall redaction rate: {overall_rate:.2f}%\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Log completion\n",
    "        logging.info(\"PDF processing completed successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error in main execution: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d6c78-7237-42bd-849d-77373a02478a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab3a9f-900b-4037-a1e1-1c26000ef569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d34b48-376a-4969-94ab-07fcc95a060f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80934945-2f6d-4290-a77d-7bbdeb039237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (redaction_env)",
   "language": "python",
   "name": "redaction_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
